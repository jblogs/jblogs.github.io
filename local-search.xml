<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>纪念Claudio Abbado</title>
    <link href="/2024/01/04/music/MemClaudioAbbado/"/>
    <url>/2024/01/04/music/MemClaudioAbbado/</url>
    
    <content type="html"><![CDATA[<p>有人说克劳迪奥阿巴多是指挥界最后一位大师，也有人不同意。</p><p>的确，他的很多演绎听起来不是很“抓耳”。</p><p>原因之一，阿巴多的录音基本上和“爆棚的音效无关”，这可能也是音乐家对录音师的要求。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Cursor编辑器使用Chatgpt API</title>
    <link href="/2023/12/05/deeplearning/Cursor_use_api/"/>
    <url>/2023/12/05/deeplearning/Cursor_use_api/</url>
    
    <content type="html"><![CDATA[<p>我最近发现 Cursor 这个编辑器很好，基于开源的 vscode，速度非常快，插件也很给力。在 MacOS 上用起来比较丝滑。</p><p>更重要的是，有了 Cursor 的账户，就可以有总共40条 GPT4 和200条&#x2F;月 GPT3.5 的AI辅助编程可以使用。无论用来 coding 还是用 markdown 写小作文，都挺好用的。——省去了切换浏览器的工作，Cursor 也会把你编辑器的上下文直接 hint 给 Chatgpt。</p><p>Cursor 的 Pro 版本，每个月$20，年付有打折。这对于 coder 来说是一个不错的选择，因为 OpenAI 支持 GPT4 的 plus 账户还要排队不是。</p><p>此外，我试了试在 Cursor 的免费版本中登记自己的 API Key，自己向 OpenAI 付费的方式。原想这种方式大概是省钱的，by token pay as you go 对不对。但是实际用下来比较氪金。因为 Cursor 总会把你当前编辑的代码作为 context token 给到 API。我看了一下 OpenAI 的 billing 记录，用量挺吓人的，特别是在一页 code 比较长的时候。你也可以选择 Return without context 省点 token 费用，但这样使用体验太差了。</p><p>结论是不要用自己的 API Key。</p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>Chatgpt</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Chatgpt,Cursor IDE,AI辅助编程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Tensorflow Dataset（数据集）</title>
    <link href="/2023/12/03/deeplearning/Tensorflow_Dataset/"/>
    <url>/2023/12/03/deeplearning/Tensorflow_Dataset/</url>
    
    <content type="html"><![CDATA[<p>TensorFlow Dataset 是 TensorFlow 中用于处理数据的模块，它提供了一种高效的数据输入管道，用于加载和预处理数据，以供模型训练和评估使用。</p><p>TensorFlow Dataset 的数据结构主要包括以下几个核心组件：</p><ol><li><p>Dataset：代表一系列元素的集合，可以是张量、numpy 数组、Python 生成器或其他数据源。</p></li><li><p>Data Transformation：用于对 Dataset 进行转换和处理的方法，例如 map、filter、batch 等，用于对数据进行预处理、筛选和批处理等操作。</p></li><li><p>Iterator：用于遍历 Dataset 中元素的迭代器，例如 iter、next 等。</p></li></ol><p>通过这些组件，TensorFlow Dataset 提供了一种灵活且高效的数据处理方式，能够方便地构建数据输入管道，加速模型训练过程。</p><h4 id="最简单的-Dataset-使用方式-Dataset-from-tensor-slices"><a href="#最简单的-Dataset-使用方式-Dataset-from-tensor-slices" class="headerlink" title="最简单的 Dataset 使用方式 Dataset.from_tensor_slices"></a>最简单的 Dataset 使用方式 Dataset.from_tensor_slices</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><br><span class="hljs-comment"># 假设有一些数据行</span><br>datarows = [[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>], <br>            [<span class="hljs-number">0.4</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.6</span>]]<br><span class="hljs-comment">#以及对应的标签</span><br>labels = [<span class="hljs-number">0</span>, <br>          <span class="hljs-number">1</span>]<br><br><span class="hljs-comment"># 创建包含元组的 Dataset</span><br>dataset = tf.data.Dataset.from_tensor_slices((datarows, labels))<br><br><span class="hljs-comment"># 打印 Dataset 中的数据</span><br><span class="hljs-keyword">for</span> data, label <span class="hljs-keyword">in</span> dataset:<br>    <span class="hljs-built_in">print</span>(data)<br>    <span class="hljs-built_in">print</span>(label)  <br>    <span class="hljs-keyword">break</span> <span class="hljs-comment">#打印一行后退出</span><br><br></code></pre></td></tr></table></figure><p>可见，本质上 Tensorflow Dataset 是一个<strong>可迭代</strong>的数据结构</p><p>没有人能够阻止我们 用 <em>take(n)</em> 拿出指定行数，因此，上面的代码可以更优雅的写为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> data, label <span class="hljs-keyword">in</span> dataset.take(<span class="hljs-number">1</span>):<br>    <span class="hljs-built_in">print</span>(data)<br>    <span class="hljs-built_in">print</span>(label)  <br></code></pre></td></tr></table></figure><h4 id="迭代-Dataset-的方式"><a href="#迭代-Dataset-的方式" class="headerlink" title="迭代 Dataset 的方式"></a>迭代 Dataset 的方式</h4><p>可以使用 Dataset 的迭代器来遍历数据集中的元素。以下是一个简单的示例代码，演示了如何创建一个迭代器并使用它来遍历数据集中的元素：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><br><span class="hljs-comment"># 创建一个简单的数据集</span><br>data = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>]<br>dataset = tf.data.Dataset.from_tensor_slices(data)<br><br><span class="hljs-comment"># 创建一个迭代器</span><br>iterator = <span class="hljs-built_in">iter</span>(dataset)<br><br><span class="hljs-comment"># 使用迭代器遍历数据集</span><br><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>    <span class="hljs-keyword">try</span>:<br>        element = <span class="hljs-built_in">next</span>(iterator)<br>        <span class="hljs-built_in">print</span>(element)<br>    <span class="hljs-keyword">except</span> StopIteration:<br>        <span class="hljs-keyword">break</span><br></code></pre></td></tr></table></figure><p>在这个示例中，我们首先创建了一个包含整数的数据集。然后，我们使用 iter 函数创建了一个迭代器，并使用 next 函数来逐个获取数据集中的元素。当数据集中的所有元素都被遍历完毕后，迭代器会抛出 StopIteration 异常，我们利用这一点来结束遍历过程。</p><p>当然，在这里，对 Dataset 使用 <em>for</em> 语句显得更直接。</p><h4 id="Dataset-一定存储-data-label-形状的元组吗？"><a href="#Dataset-一定存储-data-label-形状的元组吗？" class="headerlink" title="Dataset 一定存储 (data, label) 形状的元组吗？"></a>Dataset 一定存储 (data, label) 形状的元组吗？</h4><p>不一定，就像上面由 range 一维数组生成的 Dataset，我们并没有给他 label（标签）。</p><p>没有标签的数据集也是有用的。在机器学习和深度学习中通常用于预处理、数据增强、特征提取等操作。例如，在训练卷积神经网络时，可以使用没有标签的数据集进行数据增强，如随机裁剪、翻转、旋转等操作，以扩充训练数据集。此外，没有标签的数据集也可以用于特征提取，例如在使用预训练模型进行迁移学习时，可以使用没有标签的数据集来提取特征。</p><p>同样，我们可以给一个数据集构造多个标签列，甚至多个数据列，这也没有问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><br><span class="hljs-comment"># 创建一个包含0到9的张量的 Dataset</span><br>dataset = tf.data.Dataset.<span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>)<br><br><span class="hljs-comment"># 对 Dataset 进行转换和处理</span><br>dataset = dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: x % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>)  <span class="hljs-comment"># 筛选出偶数</span><br>dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: (x, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>))  <span class="hljs-comment"># 将每个元素与标签 0 组合</span><br><br><span class="hljs-comment"># 迭代数据</span><br><span class="hljs-keyword">for</span> data, label, c <span class="hljs-keyword">in</span> dataset:<br>    <span class="hljs-built_in">print</span>(data, label, c)<br></code></pre></td></tr></table></figure><h4 id="Dataset每次迭代出的数据结构：tf-Tensor"><a href="#Dataset每次迭代出的数据结构：tf-Tensor" class="headerlink" title="Dataset每次迭代出的数据结构：tf.Tensor"></a>Dataset每次迭代出的数据结构：tf.Tensor</h4><p>我们观察 print(data) 和 print(label) 代码返回的结果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">tf.Tensor([0.1 0.2 0.3], shape=(3,), dtype=float32) <br>tf.Tensor(0, shape=(), dtype=int32)<br></code></pre></td></tr></table></figure><p>可以看到返回的类型是 tf.Tensor，这是因为为了与 TensorFlow 的计算图和自动微分机制兼容</p><p>如果我们要拿到 numpy 数组，可以使用 <em>.numpy()</em> 方法将 Tensor 化简</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(data.numpy(), label.numpy())<br></code></pre></td></tr></table></figure><h4 id="如何构造一个自定义的-Dataset-？"><a href="#如何构造一个自定义的-Dataset-？" class="headerlink" title="如何构造一个自定义的 Dataset ？"></a>如何构造一个自定义的 Dataset ？</h4><p>我们可以用类继承的方式构造一个自己的 Dataset</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CustomDataset</span>(tf.data.Dataset):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_generator</span>(<span class="hljs-params">data, labels</span>):<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(data)):<br>            <span class="hljs-keyword">yield</span> data[i], labels[i]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__new__</span>(<span class="hljs-params">cls, data, labels</span>):<br>        <span class="hljs-keyword">return</span> tf.data.Dataset.from_generator(<br>            cls._generator,<br>            output_signature=(<br>                tf.TensorSpec(shape=(<span class="hljs-number">2</span>,), dtype=tf.float32),<br>                tf.TensorSpec(shape=(), dtype=tf.int32)<br>            ),<br>            args=(data, labels)<br>        )<br><br><span class="hljs-comment"># Example usage</span><br>input_data = [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>], [<span class="hljs-number">5</span>, <span class="hljs-number">6</span>], [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]]<br>labels = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]<br><br>custom_dataset = CustomDataset(input_data, labels)<br><br><span class="hljs-keyword">for</span> data, label <span class="hljs-keyword">in</span> custom_dataset:<br>    <span class="hljs-built_in">print</span>(data, label)<br></code></pre></td></tr></table></figure><p>关注上面的代码，派生类的构造函数 <em>__new__</em> 的实现可以看到迭代出 tf.Tensor 实例的逻辑。</p><p>那 <em>_generator</em> 函数重载里的 <em>yeild</em> 有什么意义呢？</p><p>yield 是 Python 中用于定义生成器函数的关键字。生成器函数是一种特殊的函数，它可以在迭代过程中暂停并恢复执行，从而可以逐个产生值而不需要一次性将所有值存储在内存中。</p><p>当生成器函数中包含 yield 语句时，调用该函数并不会立即执行函数体内的代码，而是返回一个生成器对象。每次调用生成器对象的 __next__ 方法（或使用 for 循环迭代生成器对象时），生成器函数会从上一次暂停的地方恢复执行，直到遇到下一个 yield 语句，然后再次暂停并将 yield 后面的值返回给调用者。</p><p>因此，yield 的作用是定义生成器函数的暂停点，并 <strong>返回一个值给调用者，同时保持函数的状态</strong> ，以便下次调用时可以从暂停的地方继续执行。这种特性使得生成器函数非常适合用于处理大量数据或需要逐个产生值的场景，因为它可以节省内存并提高效率。</p><h4 id="map、filter方法，把-Dataset-里的原始数据转化为训练程序可用的数据"><a href="#map、filter方法，把-Dataset-里的原始数据转化为训练程序可用的数据" class="headerlink" title="map、filter方法，把 Dataset 里的原始数据转化为训练程序可用的数据"></a>map、filter方法，把 Dataset 里的原始数据转化为训练程序可用的数据</h4><p>很多时候，网路上的 Dataset，或者系统提供的 Dataset，离我们要在 Tensorflow 代码中要直接用的数据表达方式是有偏差的。</p><p>例如，</p><ul><li>原始的 Dataset 可能存储的是文件名（或 URL ）作为一条记录，而我们需要的是这个文件的数据本身（或者压缩采样后的数据）作为一条记录</li><li>原始的 Dataset 的标签列为字符串类型，我们需要训练器可以理解的一个在 1 ~ 10 范围的数值编码</li></ul><p>当然，我们可以自己迭代一遍原始的 Dataset，在其间书写自己的转化逻辑。然后构造生成满足需求的，可以传递给 Keras 的数据结构。但这不够优雅。</p><p>使用 map 是一种更有效率的方式，以下是一个构造简单图片分类的 Dataset 的示例代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-comment"># 定义图片路径和标签</span><br>image_paths = [<span class="hljs-string">&quot;path_to_image1.jpg&quot;</span>, <span class="hljs-string">&quot;path_to_image2.jpg&quot;</span>, <span class="hljs-string">&quot;path_to_image3.jpg&quot;</span>]<br>labels = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]  <span class="hljs-comment"># 假设有两个类别，0 和 1</span><br><br><span class="hljs-comment"># 创建 Dataset</span><br>dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))<br><br><span class="hljs-comment"># 定义图片加载函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_and_preprocess_image</span>(<span class="hljs-params">path, label</span>):<br>    image = tf.io.read_file(path)<br>    image = tf.image.decode_jpeg(image, channels=<span class="hljs-number">3</span>)<br>    image = tf.image.resize(image, [<span class="hljs-number">228</span>, <span class="hljs-number">228</span>])  <span class="hljs-comment"># 假设图片大小为 228x228</span><br>    image = tf.cast(image, tf.float32) / <span class="hljs-number">255.0</span>  <span class="hljs-comment"># 归一化</span><br>    <span class="hljs-keyword">return</span> image, label<br><br><span class="hljs-comment"># 对 Dataset 进行转换和处理</span><br>dataset = dataset.<span class="hljs-built_in">map</span>(load_and_preprocess_image)<br><br><br><br><span class="hljs-comment"># 打印 Dataset 中的数据</span><br><span class="hljs-keyword">for</span> images, labels <span class="hljs-keyword">in</span> dataset:<br>    <span class="hljs-built_in">print</span>(images.shape, labels)<br></code></pre></td></tr></table></figure><p>我们还可以使用 <em>filter</em> 将 Dataset 中不需要的行过滤掉。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><br><span class="hljs-comment"># 创建一个包含整数的数据集</span><br>dataset = tf.data.Dataset.<span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>)<br><br><span class="hljs-comment"># 定义过滤函数，过滤偶数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">filter_even_numbers</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> x % <span class="hljs-number">2</span> == <span class="hljs-number">0</span><br><br><span class="hljs-comment"># 应用过滤函数到数据集上</span><br>filtered_dataset = dataset.<span class="hljs-built_in">filter</span>(filter_even_numbers)<br><br><span class="hljs-comment"># 打印过滤后的数据集中的元素</span><br><span class="hljs-keyword">for</span> element <span class="hljs-keyword">in</span> filtered_dataset:<br>    <span class="hljs-built_in">print</span>(element.numpy())<br></code></pre></td></tr></table></figure><h4 id="洗牌（shuffle）和-批处理（batch）"><a href="#洗牌（shuffle）和-批处理（batch）" class="headerlink" title="洗牌（shuffle）和 批处理（batch）"></a>洗牌（shuffle）和 批处理（batch）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 设置 Batch 大小并打乱数据</span><br>batch_size = <span class="hljs-number">32</span><br>dataset = dataset.shuffle(buffer_size=<span class="hljs-built_in">len</span>(image_paths)).batch(batch_size)<br><br></code></pre></td></tr></table></figure><p><em>shuffle</em> 操作的意义是不言自明的。</p><p>而 <em>batch</em> 操作会在数据集的最前面增加一个维度，通常用于表示批次（batch）的维度。假设原始数据集中的每个元素的形状是 (a, b, c)，经过 batch 操作后，每个元素的形状将变为 (batch_size, a, b, c)，其中 batch_size 表示批次的大小。 batch 操作通常用于将数据集划分为批次，以便进行批处理训练。</p><h4 id="怎么把-Dataset-进一步划为-Training-Set，Validation-Set-以及-Test-Set"><a href="#怎么把-Dataset-进一步划为-Training-Set，Validation-Set-以及-Test-Set" class="headerlink" title="怎么把 Dataset 进一步划为 Training Set，Validation Set 以及 Test Set"></a>怎么把 Dataset 进一步划为 Training Set，Validation Set 以及 Test Set</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 假设你有一个 &#x27;full_dataset&#x27;</span><br><br><span class="hljs-comment"># 洗牌</span><br>full_dataset = full_dataset.shuffle(buffer_size=<span class="hljs-number">10000</span>)<br><br><span class="hljs-comment"># 设定三个 set 的大小</span><br>train_size = <span class="hljs-built_in">int</span>(<span class="hljs-number">0.7</span> * total_size)<br>val_size = <span class="hljs-built_in">int</span>(<span class="hljs-number">0.15</span> * total_size)<br>test_size = total_size - train_size - val_size<br><br><span class="hljs-comment"># 利用 take 和 skip 来分割</span><br>train_dataset = full_dataset.take(train_size)<br>remaining_dataset = full_dataset.skip(train_size)<br>val_dataset = remaining_dataset.take(val_size)<br>test_dataset = remaining_dataset.skip(val_size)<br><br><span class="hljs-comment"># 用 repeat 给 train_dataset 一个歌曲循环属性，以免若干 epoch 后训练数据用完</span><br>train_dataset = train_dataset.repeat()<br><br><span class="hljs-comment"># 批量化</span><br>batch_size = <span class="hljs-number">32</span><br>train_dataset = train_dataset.batch(batch_size)<br>val_dataset = val_dataset.batch(batch_size)<br>test_dataset = test_dataset.batch(batch_size)<br></code></pre></td></tr></table></figure><h4 id="总结，通常这样使用-Dataset："><a href="#总结，通常这样使用-Dataset：" class="headerlink" title="总结，通常这样使用 Dataset："></a>总结，通常这样使用 Dataset：</h4><ol><li><p>创建 Dataset：使用 tf.data.Dataset 类的构造函数来创建一个 Dataset 对象，可以从张量、List、numpy 数组、文本文件、TFRecord 文件等数据源中创建 Dataset。或者从 Tensorflow 的库中装载现成的 Dataset</p></li><li><p>数据转换：通过调用 Dataset 对象的方法，如 map、filter、batch 等，对数据进行转换和处理，以便用于模型训练和评估。</p></li><li><p>迭代数据：使用迭代器（Iterator）来遍历 Dataset 中的元素，可以使用 for…in 循环或者 iter、next 方法来逐个获取数据样本。</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>Tensorflow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning,深度学习,Tensorflow 数据集,AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>关于GDP的神奇</title>
    <link href="/2023/12/01/economics/GDP/"/>
    <url>/2023/12/01/economics/GDP/</url>
    
    <content type="html"><![CDATA[<p>美国的国内生产总值（GDP）是通过国家统计局（Bureau of Economic Analysis）进行统计的。该机构使用多种方法来估算和计算GDP，包括但不限于以下几种方法：</p><ol><li><p>支出法：通过对消费、投资、政府支出和净出口等方面的支出进行统计，来估算国内生产总值。</p></li><li><p>收入法：通过对劳动报酬、利润、利息和税收等方面的收入进行统计，来估算国内生产总值。</p></li><li><p>生产法：通过对各个产业的生产价值进行统计，来估算国内生产总值。</p></li></ol><p>这些方法通常会结合使用，以确保对GDP的估算尽可能准确。同时，国家统计局也会定期对这些方法进行修订和更新，以反映经济结构和活动的变化。</p><pre><code class=" mermaid">graph LR    A(GDP) -- C --&gt; B(消费)    A -- I --&gt; C(投资)    A -- G --&gt; D(政府开支)    A -- X --&gt; E(净出口)    B -- 收入 --&gt; A    C -- 收入 --&gt; A    D -- 收入 --&gt; A    E -- 收入 --&gt; A</code></pre>]]></content>
    
    
    <categories>
      
      <category>经济学</category>
      
      <category>宏观经济学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>曼昆,经济学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>神奇的菲利普斯曲线</title>
    <link href="/2023/11/30/economics/%E7%A5%9E%E5%A5%87%E7%9A%84%E8%8F%B2%E5%88%A9%E6%99%AE%E6%96%AF%E6%9B%B2%E7%BA%BF/"/>
    <url>/2023/11/30/economics/%E7%A5%9E%E5%A5%87%E7%9A%84%E8%8F%B2%E5%88%A9%E6%99%AE%E6%96%AF%E6%9B%B2%E7%BA%BF/</url>
    
    <content type="html"><![CDATA[<h3 id="神奇的菲利普斯曲线"><a href="#神奇的菲利普斯曲线" class="headerlink" title="神奇的菲利普斯曲线"></a>神奇的菲利普斯曲线</h3><p>菲利普斯曲线公式如下：</p><span id="more"></span><p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="22.134ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 9783.2 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="mi" transform="translate(603,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(1186,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(2241.8,0)"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="TeXAtom" transform="translate(603,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(361,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1139,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(4276,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(5276.2,0)"><path data-c="1D70C" d="M58 -216Q25 -216 23 -186Q23 -176 73 26T127 234Q143 289 182 341Q252 427 341 441Q343 441 349 441T359 442Q432 442 471 394T510 276Q510 219 486 165T425 74T345 13T266 -10H255H248Q197 -10 165 35L160 41L133 -71Q108 -168 104 -181T92 -202Q76 -216 58 -216ZM424 322Q424 359 407 382T357 405Q322 405 287 376T231 300Q217 269 193 170L176 102Q193 26 260 26Q298 26 334 62Q367 92 389 158T418 266T424 322Z"></path></g><g data-mml-node="mo" transform="translate(5793.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(6182.2,0)"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(7314.7,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(8314.9,0)"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(9394.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></p><p>其中，</p><ul><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.055ex" height="1.332ex" role="img" focusable="false" viewBox="0 -431 908.3 588.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="mi" transform="translate(603,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></g></svg></mjx-container> 是 t 期的通货膨胀率，</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.471ex;" xmlns="http://www.w3.org/2000/svg" width="4.099ex" height="1.446ex" role="img" focusable="false" viewBox="0 -431 1811.9 639"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="TeXAtom" transform="translate(603,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(361,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1139,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></g></svg></mjx-container> 是 t-1 期的通货膨胀率，</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.059ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 910.3 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></g></svg></mjx-container> 是 t 期的失业率，</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.442ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 1079.3 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container> 是自然失业率，</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex;" xmlns="http://www.w3.org/2000/svg" width="1.17ex" height="1.489ex" role="img" focusable="false" viewBox="0 -442 517 658"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D70C" d="M58 -216Q25 -216 23 -186Q23 -176 73 26T127 234Q143 289 182 341Q252 427 341 441Q343 441 349 441T359 442Q432 442 471 394T510 276Q510 219 486 165T425 74T345 13T266 -10H255H248Q197 -10 165 35L160 41L133 -71Q108 -168 104 -181T92 -202Q76 -216 58 -216ZM424 322Q424 359 407 382T357 405Q322 405 287 376T231 300Q217 269 193 170L176 102Q193 26 260 26Q298 26 334 62Q367 92 389 158T418 266T424 322Z"></path></g></g></g></svg></mjx-container> 是反应系数。</li></ul><p><strong>也就是说非恶性的通货膨胀和失业率呈反作用关系</strong></p>]]></content>
    
    
    <categories>
      
      <category>经济学</category>
      
      <category>宏观经济学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>曼昆,经济学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>略读曼昆《宏观经济学》第十版</title>
    <link href="/2023/11/30/economics/%E7%95%A5%E8%AF%BB%E6%9B%BC%E6%98%86%E3%80%8A%E5%AE%8F%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%AD%A6%E3%80%8B%E7%AC%AC%E5%8D%81%E4%B8%80%E7%89%88/"/>
    <url>/2023/11/30/economics/%E7%95%A5%E8%AF%BB%E6%9B%BC%E6%98%86%E3%80%8A%E5%AE%8F%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%AD%A6%E3%80%8B%E7%AC%AC%E5%8D%81%E4%B8%80%E7%89%88/</url>
    
    <content type="html"><![CDATA[<h3 id="略读曼昆《宏观经济学》第十人大版-及英文版第11版"><a href="#略读曼昆《宏观经济学》第十人大版-及英文版第11版" class="headerlink" title="略读曼昆《宏观经济学》第十人大版 及英文版第11版"></a>略读曼昆《宏观经济学》第十人大版 及英文版第11版</h3><p><strong>以下文字为 Cursor.so 的 gpt-4 生成</strong></p><p>在阅读曼昆的《宏观经济学》第十版后，我深感其深厚的学术底蕴和严谨的逻辑推理。这本书以其独特的视角，为我们揭示了宏观经济学的核心理念和基本框架。</p><p>曼昆的语言清晰，逻辑严密，使得复杂的经济理论变得易于理解。他的观点既有深度，又不失广度，使得读者能够全面地理解和掌握宏观经济学的基本原理。</p><p>然而，尽管这本书的内容丰富，但并不意味着它是无懈可击的。在某些章节中，我发现曼昆的观点有时过于理论化，缺乏对现实世界复杂性的充分认识。此外，他的一些论述也可能过于依赖数学模型，而忽视了经济学的社会和历史背景。</p><p>总的来说，曼昆的《宏观经济学》是一本值得一读的经典之作。它既可以作为经济学入门读物，也可以作为经济学者深入研究的参考书。虽然它并非完美，但它的优点远远超过了其缺点，使得我们能够更好地理解和应用宏观经济学的知识。</p><pre><code class="mermaid">stateDiagram-v2    [*] --&gt; Still    Still --&gt; [*]    Still --&gt; Moving    Moving --&gt; Still    Moving --&gt; Crash    Crash --&gt; [*]</code></pre>]]></content>
    
    
    <categories>
      
      <category>经济学</category>
      
      <category>宏观经济学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>曼昆,经济学</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
